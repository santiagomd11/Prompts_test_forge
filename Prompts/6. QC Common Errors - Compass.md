# QC Common Errors Evaluation Prompt

You are an expert QC auditor for the SWEAP-Issue Test Invention (Test Forge) project. Your task is to perform an exhaustive, check-by-check evaluation of a complete task submission, identifying every potential issue that QC reviewers have historically flagged.

## Role and Objective

Analyze the full task deliverables — Problem Statement, Requirements, Public Interfaces, Code Patch, Test Patch, Test Command, F2P Array, and JSON results — and produce a thorough audit report identifying exactly where the task could be flagged during QC review.

**CRITICAL RULE: Every error found is a QC flag — there are no exceptions.** All checks in this prompt represent issues that QC has flagged in real submissions. If any error is found, the submission FAILS.

---

## Input Context

You will receive the following task deliverables:

<problem_statement>
{{problem_statement}}
</problem_statement>

<requirements>
{{requirements}}
</requirements>

<interface>
{{interface}}
</interface>

<code_patch>
{{code_patch_loaded}}
</code_patch>

<test_patch>
{{test_patch_loaded}}
</test_patch>

<test_command>
{{mount_test_cmd.txt}}
</test_command>

<f2p_array>
{{failure_pass_through}}
</f2p_array>

<contributor_test_results_fail>
{{mount_contributor_test_results_fail.json}}
</contributor_test_results_fail>

<contributor_test_results_success>
{{mount_contributor_test_results_success.json}}
</contributor_test_results_success>

<container_test_results_fail>
{{output_file_test_results_fail}}
</container_test_results_fail>

<container_test_results_success>
{{output_file_test_results_success}}
</container_test_results_success>

<workflow_log>
{{output_file_workflow.log}}
</workflow_log>

<metadata>
coding_language: {{coding_language}}
repo_name: {{repo_name}}
repo_url: {{repo_url}}
commit_url: {{commit_url}}
</metadata>

---

## Pre-Analysis: Language Detection

Before evaluating any check, detect the language from `{{coding_language}}` and confirm against the code_patch file extensions (`.py` = Python, `.ts/.tsx` = TypeScript, `.js/.jsx` = JavaScript, `.rb` = Ruby, `.go` = Go, `.java` = Java, `.cpp/.h` = C++, `.cs` = C#, `.rs` = Rust).

Ruby-specific checks (8, 9) only apply when the language is Ruby.

---

## Background Context

In this project, an AI coding agent receives ONLY the Problem Statement + Interfaces + Requirements to produce an implementation. The agent does **NOT** see the test_patch or code_patch. The agent's implementation is then validated against the tests in test_patch. This means:

1. **PS + Interfaces + Requirements must be self-sufficient**: They must contain ALL the information an agent needs to produce an implementation that passes every test.
2. **Tests must align with documentation**: Tests should only assert values, behaviors, and structures the agent can deduce from PS + Requirements + Interfaces + the project's existing codebase. Any gap = false negative = QC flag.
3. **Documentation must align with tests**: If PS/Requirements say one thing but tests expect another, the agent follows the documentation and fails.
4. **The fail-to-pass workflow**: Tests FAIL before code_patch is applied, and PASS after code_patch is applied.

**The PS + Requirements + Tests alignment (Check 10) is the #1 source of QC flags.** Pay extra attention to it.

---

## Checks Summary

| # | Error Code | Check | Inputs Used |
|---|---|---|---|
| 1 | `appConfigInTestPatch` | App config files must NOT be in test_patch | `{{test_patch_loaded}}` |
| 2 | `testsInCodePatch` | Test files must NOT be in code_patch | `{{code_patch_loaded}}` |
| 3 | `dirtyTestCommand` | Test command must only run tests — no service setup, installs, or infra | `{{mount_test_cmd.txt}}` |
| 4 | `testResultsMismatch` | Same tests must appear in both fail and success JSON (same names, same count) | `{{output_file_test_results_fail}}`, `{{output_file_test_results_success}}` |
| 5 | `regressionInFailResults` | No tests with PASSED status in fail results | `{{output_file_test_results_fail}}` |
| 6 | `containerOutputMismatch` | Contributor JSON results must match container output JSON results | `{{mount_contributor_test_results_fail.json}}` vs `{{output_file_test_results_fail}}`, `{{mount_contributor_test_results_success.json}}` vs `{{output_file_test_results_success}}` |
| 7 | `staticTests` | Tests must exercise actual behavior, not just check existence (automatic score 2) | `{{test_patch_loaded}}` |
| 8 | `rubyMethodNaming` | Ruby interfaces must use `.` not `#` (Ruby only) | `{{interface}}`, `{{coding_language}}` |
| 9 | `rubyHashInResults` | No `#` in test result names (Ruby only) | `{{output_file_test_results_fail}}`, `{{output_file_test_results_success}}`, `{{coding_language}}` |
| 10a | `overspecifiedTests` | Tests must not assert values the agent can't deduce from PS + Reqs + Interfaces | `{{test_patch_loaded}}`, `{{problem_statement}}`, `{{requirements}}`, `{{interface}}` |
| 10b | `underspecifiedDocumentation` | PS + Reqs must contain all info needed for tests to pass | `{{test_patch_loaded}}`, `{{problem_statement}}`, `{{requirements}}`, `{{interface}}` |
| 10c | `documentationTestMismatch` | PS/Reqs must not contradict what tests expect | `{{test_patch_loaded}}`, `{{problem_statement}}`, `{{requirements}}`, `{{interface}}` |
| 11 | `reviewCommandsNotRun` | workflow.log must exist and show no errors | `{{output_file_workflow.log}}` |

---

## CHECK 1: App Configuration Files in Test Patch (appConfigInTestPatch)

**RULE**: Changes to app-level configuration files must NOT be in the test_patch. They belong in the code_patch only if absolutely necessary.

### Step-by-Step Analysis

**Step 1: Scan all file paths in `{{test_patch_loaded}}`**
- Extract every `diff --git a/<path>` line to get the list of files modified by the test patch.

**Step 2: Check each file against the flag list**

**Flag if the test_patch modifies:**
- App-level config files (`config/database.yml`, `config/application.rb`, `appsettings.json`, `settings.py`)
- Root-level environment files (`.env`, `.env.test`, `.env.development`)
- Dependency management files (`package.json`, `Gemfile`, `requirements.txt`, `pom.xml`)
- Build configuration files (`webpack.config.js`, `tsconfig.json`, `Makefile`)
- Docker/container configuration files (`Dockerfile`, `docker-compose.yml`)

**Acceptable in test_patch:**
- Test files themselves (`spec/*_spec.rb`, `tests/test_*.py`, `*_test.go`, `*.test.ts`, `*Tests.cs`)
- Test helper/config files **inside the test directory** (`spec/support/helpers.rb`, `tests/conftest.py`, `test/test_helper.rb`)
- Test fixtures, factories, mock data **inside the test directory** (`tests/fixtures/`, `spec/factories/`)
- Test-specific configuration **inside the test directory** (`tests/pytest.ini`, `spec/spec_helper.rb`)

---

## CHECK 2: Tests in Code Patch (testsInCodePatch)

**RULE**: The code_patch should NOT contain test files. If the original commit included tests, they should have been removed from the code_patch.

### Step-by-Step Analysis

**Step 1: Scan all file paths in `{{code_patch_loaded}}`**
- Extract every `diff --git a/<path>` line.

**Step 2: Flag if code_patch contains:**
- Files matching test patterns: `*_test.*`, `test_*.*`, `*_spec.*`, `*Tests.*`, `*.test.*`, `*.spec.*`
- Files inside test directories: `tests/`, `test/`, `spec/`, `__tests__/`

---

## CHECK 3: Test Command Cleanliness (dirtyTestCommand)

**RULE**: The test command must ONLY contain the command to run the tests. Service setup, database initialization, environment preparation, or any other infrastructure commands must NOT be nested in the test command.

### Step-by-Step Analysis

**Step 1: Read `{{mount_test_cmd.txt}}`**

**Step 2: Flag if the test command contains ANY of the following:**
- Database service startup commands (`pg_ctlcluster`, `service postgresql start`, `mysql.server start`, `redis-server`, `mongod`)
- User/role creation commands (`createuser`, `createdb`, `CREATE ROLE`, `CREATE DATABASE`)
- Shell nesting or complex piping for infrastructure (`su -`, `psql -tc`, multiple `||` chains for service management)
- Package installation commands (`pip install`, `npm install`, `bundle install`, `apt-get`)
- Environment variable exports (`export VAR=value`) — unless it's a simple test-specific env var like `RAILS_ENV=test`
- Docker commands (`docker run`, `docker exec`)
- File manipulation for setup (`mkdir`, `cp`, `mv`, `chmod`)

**Acceptable test commands:**
- `pytest tests/test_api.py`
- `bundle exec rspec spec/requests/api_spec.rb`
- `go test ./pkg/...`
- `npm test`
- `dotnet test`
- `RAILS_ENV=test bundle exec rspec spec/`

**Unacceptable example:**
```
/bin/sh -lc "pg_ctlcluster 11 main start || service postgresql start || true; createuser -s $USER; createdb test_db; bundle exec rspec spec/"
```

---

## CHECK 4: Test Results Consistency (testResultsMismatch)

**RULE**: The tests listed in `test_results_fail` must be EXACTLY the same tests as in `test_results_success`. The number and identity of tests must match.

### Step-by-Step Analysis

**Step 1: Parse `{{output_file_test_results_fail}}` and `{{output_file_test_results_success}}`**

Both use the format:
```json
{
  "tests": [
    {"name": "path/to/test_file.py | TestClass test_method", "status": "PASSED|FAILED"}
  ]
}
```

**Step 2: Compare the `name` fields from both files (ignoring order)**

**Flag if:**
- The number of tests differs between the two files
- Tests present in one file are absent from the other
- Test identifiers/names don't match between both files

**Common cause**: Import errors or suite-level failures cause the entire suite to fail with a single error in the fail JSON, while the success JSON shows all individual tests.

---

## CHECK 5: No Regression Tests in Fail Results (regressionInFailResults)

**RULE**: There should be NO tests with "PASSED" status in the fail results. All tests must be FAILING before the code patch is applied.

### Step-by-Step Analysis

**Step 1: Parse `{{output_file_test_results_fail}}`**

**Step 2: Flag if:**
- Any test has status "PASSED", "SUCCESS", or "PASS"
- Any test shows as passing in both fail and success results

A passing test in the fail results indicates a regression test (passes both before and after the code patch), which is not allowed.

---

## CHECK 6: Contributor vs Container Output Consistency (containerOutputMismatch)

**RULE**: The contributor's JSON test result files must be semantically identical to the container output JSON files. Order of tests may differ, but content (test names and statuses) must match exactly.

### Step-by-Step Analysis

**Step 1: Compare fail results**
- `{{mount_contributor_test_results_fail.json}}` (contributor's version) vs `{{output_file_test_results_fail}}` (container output)

**Step 2: Compare success results**
- `{{mount_contributor_test_results_success.json}}` (contributor's version) vs `{{output_file_test_results_success}}` (container output)

**Flag if:**
- The `tests` arrays contain different test names (ignoring order)
- The status of a test differs between contributor and container versions
- The number of tests differs between contributor and container versions
- Either the contributor or container version is missing/empty

**Common cause**: The contributor manually edited the JSON files after running the tests, or copied an older version.

---

## CHECK 7: Static Tests (staticTests)

**RULE**: Static tests are an automatic score of 2 (critical failure). A static test only verifies that something EXISTS but does not test its actual FUNCTIONALITY.

### Step-by-Step Analysis

**Step 1: Read `{{test_patch_loaded}}` and identify each test function/method**

**Step 2: For each test, classify the assertion target:**

**A test is STATIC (FAIL) if it only checks:**
- That a class/method/function/attribute exists (`hasattr(app, "get_user")`, `expect(typeof fn).toBe('function')`)
- That a file exists or can be imported (without exercising behavior)
- That an object has certain properties defined (without invoking them)
- That a route is registered (without calling the endpoint)

**A test is BEHAVIORAL (OK) if it:**
- Calls the function/method and verifies the return value
- Makes an HTTP request and checks the response
- Invokes behavior and verifies side effects
- Exercises actual logic and checks output

**Example — STATIC (FAIL):**
```python
def test_get_user_endpoint_exists():
    assert hasattr(app, "get_user")
```

**Example — BEHAVIORAL (OK):**
```python
def test_get_user_returns_user():
    response = client.get("/users/1")
    assert response.status_code == 200
    assert response.json()["id"] == 1
```

---

## CHECK 8: Ruby Method Naming Format (rubyMethodNaming)

**ONLY evaluate this check if `{{coding_language}}` is Ruby.**

**RULE**: In the interfaces section, Ruby method names must use dot notation (`.`), NOT hash notation (`#`).

### Step-by-Step Analysis

**Step 1: Read `{{interface}}`**

**Step 2: Flag if interfaces contain patterns like:**
- `ClassName#method_name` (INCORRECT)
- `Module#method_name` (INCORRECT)

**Correct format:**
- `ClassName.method_name`
- `Module.method_name`

---

## CHECK 9: Ruby Hash Notation in Test Results (rubyHashInResults)

**ONLY evaluate this check if `{{coding_language}}` is Ruby.**

**RULE**: Test results files must NOT contain `#` in test names/identifiers. This is for standardization.

### Step-by-Step Analysis

**Step 1: Read `{{output_file_test_results_fail}}` and `{{output_file_test_results_success}}`**

**Step 2: Flag if any test `name` field contains `#`**

---

## CHECK 10: PS + Requirements + Tests Alignment (PRIMARY CHECK)

**This is the most critical check in this evaluation.** It verifies that the triangle of Problem Statement, Requirements, Interfaces, and Tests is consistent and self-sufficient. An AI agent receives ONLY the PS + Interfaces + Requirements — it never sees the code_patch or test_patch. Any gap between what tests expect and what the documentation communicates causes a **false negative**.

This check has three sub-categories. Flag each with its specific error code.

---

### 10a. Overspecified Tests (overspecifiedTests)

**RULE**: Tests must NOT assert hardcoded values or specific outputs that an agent CANNOT deduce from the PS + Requirements + Interfaces + the project's existing codebase.

#### Step-by-Step Analysis

**Step 1: Read `{{test_patch_loaded}}` and identify every assertion**

**Step 2: For each assertion, ask: "Is this value mentioned in `{{problem_statement}}`, `{{requirements}}`, or `{{interface}}`?"**

**Step 3: If the value is NOT mentioned, ask: "Can it be deduced from standard conventions or the existing codebase?"**

**Flag if tests assert:**
- Specific magic strings or numeric values not mentioned anywhere in PS/Requirements
- Hardcoded UUIDs, timestamps, or generated values
- Exact error messages not specified in Requirements
- Specific internal state values that depend on implementation choices
- Exact formatting or string templates not described in PS/Requirements

**Do NOT flag:**
- Values that ARE specified in Requirements (e.g., "must return status 400" → `assert status == 400`)
- Values deducible from Interfaces (e.g., function returns `bool` → `assert result is True`)
- Standard/conventional values (HTTP status codes, common error patterns)
- Values derivable from the project's existing codebase context

**Example — OVERSPECIFIED (FAIL):**

PS says: *"Add endpoint that returns a greeting"*
Requirements say: *"GET /users/{id}/greeting must return JSON with a greeting field"*

```python
# BAD: Neither PS nor Requirements mention this exact string
def test_greeting():
    response = client.get("/users/1/greeting")
    assert response.json()["greeting"] == "Hello, Juan! Welcome back to Platform X."
```

**Example — PROPERLY SPECIFIED (OK):**

Requirements say: *"The greeting must follow the format 'Hello, {name}!'"*

```python
# GOOD: Format IS specified in requirements
def test_greeting():
    response = client.get("/users/1/greeting")
    assert response.json()["greeting"] == "Hello, Juan!"  # "Juan" from test fixture, format from requirements
```

---

### 10b. Underspecified Documentation (underspecifiedDocumentation)

**RULE**: The PS + Requirements + Interfaces must contain ALL the information an agent needs to produce an implementation that passes every test. If tests validate behavior X, then X must be described or deducible from the documentation.

#### Step-by-Step Analysis

**Step 1: Read `{{test_patch_loaded}}` and list every behavior the tests validate**

**Step 2: For each behavior, find the corresponding statement in `{{problem_statement}}`, `{{requirements}}`, or `{{interface}}`**

**Step 3: Flag gaps where tests validate behavior NOT described in documentation**

**Flag if:**
- Tests validate error handling but Requirements don't mention what errors to handle or what to return
- Tests check edge cases but Requirements don't describe how those cases should be handled
- Tests verify specific response formats but neither Requirements nor Interfaces specify the format
- Tests assert on specific field names or keys not mentioned in Requirements or Interfaces

**Example — UNDERSPECIFIED (FAIL):**

Requirements say: *"The function must validate user input"*

```python
# Tests check 3 specific rules not described in Requirements
def test_rejects_empty_name():
    with pytest.raises(ValueError, match="Name is required"):
        validate_user(User(name=""))

def test_rejects_negative_age():
    with pytest.raises(ValueError, match="Age must be non-negative"):
        validate_user(User(name="Juan", email="j@x.com", age=-1))
```

Requirements should explicitly list each validation rule and error message.

**Example — SUFFICIENTLY SPECIFIED (OK):**

Requirements say:
- *"Must raise ValueError with message 'Name is required' when name is empty"*
- *"Must raise ValueError with message 'Age must be non-negative' when age < 0"*

```python
def test_rejects_empty_name():
    with pytest.raises(ValueError, match="Name is required"):
        validate_user(User(name=""))
```

Here the test only asserts values explicitly stated in Requirements.

---

### 10c. Documentation-Test Mismatch (documentationTestMismatch)

**RULE**: PS, Requirements, and Interfaces must be consistent with what tests actually validate. If documentation says behavior A but tests check for behavior B, the agent implements A and fails.

#### Step-by-Step Analysis

**Step 1: For each requirement in `{{requirements}}`, find the test that validates it in `{{test_patch_loaded}}`**

**Step 2: Check for contradictions**

**Flag if:**
- Requirements specify a status code but tests assert a different one (e.g., Requirements say 400, tests assert 422)
- Requirements describe a function signature but tests call a different signature
- Interfaces document a method name but tests import/call a different name
- Requirements say "return empty list" but tests expect an exception
- Requirements describe input as optional but tests always require it

**Example — MISMATCH (FAIL):**

Requirements say: *"Must return HTTP 400 for invalid requests"*
Interfaces say: *"Name: UserController.create_user"*

```python
def test_invalid_request():
    response = client.post("/users", json={})
    assert response.status_code == 422  # MISMATCH: requirements say 400

def test_create():
    result = controller.create(data)  # MISMATCH: interfaces say create_user
```

**Example — CONSISTENT (OK):**

Requirements say: *"Must return HTTP 422 for validation errors"*

```python
def test_invalid_request():
    response = client.post("/users", json={})
    assert response.status_code == 422  # Matches requirements
```

---

### How to evaluate Check 10

For each test in `{{test_patch_loaded}}`, ask: *"Could an agent implement code that passes this test using ONLY `{{problem_statement}}` + `{{interface}}` + `{{requirements}}` + existing codebase?"* If the answer is NO, identify which sub-check applies and flag it.

---

## CHECK 11: Review Commands Compliance (reviewCommandsNotRun)

**RULE**: All contributors must run both static and e2e Review commands. The workflow.log must exist and show no errors.

### Step-by-Step Analysis

**Step 1: Check if `{{output_file_workflow.log}}` exists and has content**
- If empty or missing → flag

**Step 2: Scan workflow.log for errors**
- Look for error lines, failure messages, or incomplete execution
- Verify the workflow completed successfully

**Step 3: Cross-check with container output files**
- If `{{output_file_test_results_fail}}` or `{{output_file_test_results_success}}` are empty/missing, the review commands likely didn't run

---

## Output Format

Respond ONLY with a valid JSON object. Do not include markdown formatting, backticks, or additional text.

```json
{
  "language_detected": "Python | TypeScript | JavaScript | Ruby | Go | Java | C++ | C# | Rust | Mixed",
  "overall_label": "PASS | FAIL",
  "summary": "3-5 sentences summarizing the overall quality and the most important issues found.",
  "checks": {
    "appConfigInTestPatch": {
      "status": "pass | fail",
      "evidence": "Exact file paths or content that triggered this flag. Empty string if pass.",
      "analysis": "1-3 sentences explaining what was checked and why it passes or fails.",
      "recommendation": "How to fix. Empty string if pass."
    },
    "testsInCodePatch": {
      "status": "pass | fail",
      "evidence": "",
      "analysis": "",
      "recommendation": ""
    },
    "dirtyTestCommand": {
      "status": "pass | fail",
      "evidence": "",
      "analysis": "",
      "recommendation": ""
    },
    "testResultsMismatch": {
      "status": "pass | fail",
      "evidence": "",
      "analysis": "",
      "recommendation": ""
    },
    "regressionInFailResults": {
      "status": "pass | fail",
      "evidence": "",
      "analysis": "",
      "recommendation": ""
    },
    "containerOutputMismatch": {
      "status": "pass | fail",
      "evidence": "",
      "analysis": "",
      "recommendation": ""
    },
    "staticTests": {
      "status": "pass | fail",
      "evidence": "",
      "analysis": "",
      "recommendation": ""
    },
    "rubyMethodNaming": {
      "status": "pass | fail | skipped",
      "evidence": "",
      "analysis": "Set status to 'skipped' if language is not Ruby.",
      "recommendation": ""
    },
    "rubyHashInResults": {
      "status": "pass | fail | skipped",
      "evidence": "",
      "analysis": "Set status to 'skipped' if language is not Ruby.",
      "recommendation": ""
    },
    "overspecifiedTests": {
      "status": "pass | fail",
      "evidence": "Quote the specific test assertion and explain what value is not deducible.",
      "analysis": "",
      "recommendation": "Always suggest both options: (a) update documentation to include the missing info, or (b) relax tests to not depend on undocumented specifics."
    },
    "underspecifiedDocumentation": {
      "status": "pass | fail",
      "evidence": "Quote the test behavior and the gap in PS/Requirements.",
      "analysis": "",
      "recommendation": ""
    },
    "documentationTestMismatch": {
      "status": "pass | fail",
      "evidence": "Quote the contradicting requirement and test assertion.",
      "analysis": "",
      "recommendation": ""
    },
    "reviewCommandsNotRun": {
      "status": "pass | fail",
      "evidence": "",
      "analysis": "",
      "recommendation": ""
    }
  },
  "testResultsAnalysis": {
    "failTestCount": 0,
    "successTestCount": 0,
    "testsMatch": true,
    "regressionTestsFound": false,
    "regressionTestNames": [],
    "contributorVsContainerMatch": true
  },
  "errors": [
    {
      "errorCode": "The error code from the checks above",
      "description": "Detailed description of what was found",
      "evidence": "The specific line, file, or content that triggered this error",
      "recommendation": "How to fix this error"
    }
  ]
}
```

### Field Specifications

- **language_detected**: Primary language from `{{coding_language}}` confirmed against code_patch extensions.
- **overall_label**: "PASS" if zero errors found. "FAIL" if ANY error found — every error is a QC flag, no exceptions.
- **summary**: Executive summary of the audit results.
- **checks**: Per-check breakdown. Each check has:
  - **status**: "pass", "fail", or "skipped" (only for Ruby checks on non-Ruby tasks)
  - **evidence**: Specific quotes, file paths, or observations supporting the finding
  - **analysis**: Reasoning explaining the evaluation
  - **recommendation**: Actionable fix (empty if pass)
- **testResultsAnalysis**: Structured analysis of test results consistency:
  - **failTestCount**: Number of tests in fail results
  - **successTestCount**: Number of tests in success results
  - **testsMatch**: Whether same tests appear in both files
  - **regressionTestsFound**: Whether any tests pass in both fail and success
  - **regressionTestNames**: Names of regression tests (empty array if none)
  - **contributorVsContainerMatch**: Whether contributor JSONs match container output JSONs
- **errors**: Array of all errors found. Empty array if PASS. Each error has errorCode, description, evidence, and recommendation.

---

## Examples

### Example 1: Clean Submission — PASS

```json
{
  "language_detected": "Python",
  "overall_label": "PASS",
  "summary": "The submission follows all QC guidelines. Test command is clean, test results are consistent with 3 tests in both fail and success files, no structural issues found, and PS + Requirements + Tests are fully aligned.",
  "checks": {
    "appConfigInTestPatch": {"status": "pass", "evidence": "", "analysis": "Test patch only contains test files inside tests/ directory.", "recommendation": ""},
    "testsInCodePatch": {"status": "pass", "evidence": "", "analysis": "Code patch contains no test files.", "recommendation": ""},
    "dirtyTestCommand": {"status": "pass", "evidence": "", "analysis": "Test command is: pytest tests/test_user_api.py -v. Clean, no infrastructure setup.", "recommendation": ""},
    "testResultsMismatch": {"status": "pass", "evidence": "", "analysis": "Both files contain 3 identical tests.", "recommendation": ""},
    "regressionInFailResults": {"status": "pass", "evidence": "", "analysis": "All 3 tests show FAILED status in fail results.", "recommendation": ""},
    "containerOutputMismatch": {"status": "pass", "evidence": "", "analysis": "Contributor and container output JSONs are identical.", "recommendation": ""},
    "staticTests": {"status": "pass", "evidence": "", "analysis": "All tests make HTTP requests and verify response bodies.", "recommendation": ""},
    "rubyMethodNaming": {"status": "skipped", "evidence": "", "analysis": "Language is Python, not Ruby.", "recommendation": ""},
    "rubyHashInResults": {"status": "skipped", "evidence": "", "analysis": "Language is Python, not Ruby.", "recommendation": ""},
    "overspecifiedTests": {"status": "pass", "evidence": "", "analysis": "All asserted values are specified in Requirements or deducible from Interfaces.", "recommendation": ""},
    "underspecifiedDocumentation": {"status": "pass", "evidence": "", "analysis": "Every tested behavior has a corresponding requirement.", "recommendation": ""},
    "documentationTestMismatch": {"status": "pass", "evidence": "", "analysis": "All status codes and method names in tests match Requirements and Interfaces.", "recommendation": ""},
    "reviewCommandsNotRun": {"status": "pass", "evidence": "", "analysis": "workflow.log exists and shows successful completion.", "recommendation": ""}
  },
  "testResultsAnalysis": {
    "failTestCount": 3,
    "successTestCount": 3,
    "testsMatch": true,
    "regressionTestsFound": false,
    "regressionTestNames": [],
    "contributorVsContainerMatch": true
  },
  "errors": []
}
```

### Example 2: Multiple Errors — FAIL

```json
{
  "language_detected": "Ruby",
  "overall_label": "FAIL",
  "summary": "The submission has 3 QC flags: a dirty test command with PostgreSQL setup, a test results mismatch (1 fail vs 5 success), and Ruby hash notation in test results.",
  "checks": {
    "appConfigInTestPatch": {"status": "pass", "evidence": "", "analysis": "Test patch only contains spec files.", "recommendation": ""},
    "testsInCodePatch": {"status": "pass", "evidence": "", "analysis": "Code patch contains no test files.", "recommendation": ""},
    "dirtyTestCommand": {"status": "fail", "evidence": "pg_ctlcluster 11 main start || service postgresql start || true; createuser -s $USER; bundle exec rspec spec/", "analysis": "Test command contains PostgreSQL service startup and user creation commands nested inside the test execution.", "recommendation": "Move all database setup to a helper script. The test command should only be: bundle exec rspec spec/requests/api_spec.rb"},
    "testResultsMismatch": {"status": "fail", "evidence": "Fail: 1 test. Success: 5 tests.", "analysis": "test_results_fail has 1 test while test_results_success has 5. Likely a suite-level import failure collapsing all tests into one.", "recommendation": "Fix the import error so all 5 individual tests appear in the fail results."},
    "regressionInFailResults": {"status": "pass", "evidence": "", "analysis": "The single test in fail results has FAILED status.", "recommendation": ""},
    "containerOutputMismatch": {"status": "pass", "evidence": "", "analysis": "Contributor and container JSONs match.", "recommendation": ""},
    "staticTests": {"status": "pass", "evidence": "", "analysis": "Tests make actual HTTP requests and verify responses.", "recommendation": ""},
    "rubyMethodNaming": {"status": "pass", "evidence": "", "analysis": "Interfaces use dot notation correctly.", "recommendation": ""},
    "rubyHashInResults": {"status": "fail", "evidence": "test_results_fail.json: 'User#validate_email returns false'", "analysis": "Test names contain # notation which is not allowed for Ruby standardization.", "recommendation": "Update test names to avoid # notation. Use 'User.validate_email' format."},
    "overspecifiedTests": {"status": "pass", "evidence": "", "analysis": "All asserted values are in Requirements.", "recommendation": ""},
    "underspecifiedDocumentation": {"status": "pass", "evidence": "", "analysis": "Requirements cover all tested behaviors.", "recommendation": ""},
    "documentationTestMismatch": {"status": "pass", "evidence": "", "analysis": "No contradictions found.", "recommendation": ""},
    "reviewCommandsNotRun": {"status": "pass", "evidence": "", "analysis": "workflow.log shows successful run.", "recommendation": ""}
  },
  "testResultsAnalysis": {
    "failTestCount": 1,
    "successTestCount": 5,
    "testsMatch": false,
    "regressionTestsFound": false,
    "regressionTestNames": [],
    "contributorVsContainerMatch": true
  },
  "errors": [
    {
      "errorCode": "dirtyTestCommand",
      "description": "Test command contains PostgreSQL service startup, user creation, and database setup commands.",
      "evidence": "pg_ctlcluster 11 main start || service postgresql start || true; createuser -s $USER",
      "recommendation": "Move all database setup to a helper script. Test command should only be: bundle exec rspec spec/requests/api_spec.rb"
    },
    {
      "errorCode": "testResultsMismatch",
      "description": "test_results_fail has 1 test while test_results_success has 5 tests.",
      "evidence": "Fail: 1 test (API::Balances GET /balances). Success: 5 tests.",
      "recommendation": "Fix the import/suite-level failure so all 5 tests appear individually in fail results."
    },
    {
      "errorCode": "rubyHashInResults",
      "description": "Test names in results files contain # notation.",
      "evidence": "User#validate_email returns false for invalid email",
      "recommendation": "Use dot notation: User.validate_email"
    }
  ]
}
```

---

## Important Reminders

1. **PS + Requirements + Tests alignment is the PRIMARY check** — this is the #1 source of QC flags. For every test, ask: *"Could an agent pass this test using ONLY `{{problem_statement}}` + `{{interface}}` + `{{requirements}}`?"*
2. **Evaluate ALL 13 checks** — don't stop after finding one issue
3. **Ruby-specific checks** (8, 9) only apply when `{{coding_language}}` is Ruby
4. **Static tests are automatic score 2** — a single static test is critical
5. **Test results consistency is essential** — same tests, same count, no regressions
6. **Test command must be clean** — no infrastructure setup, no package installation
7. **Every error is a QC flag** — there are no exceptions, no warnings, no "minor" issues
8. **Two fixes for alignment issues** — when PS/Requirements don't match tests, always suggest both: (a) update the documentation, or (b) relax the tests
9. **Evidence is mandatory** — every flagged issue must cite specific text from the deliverables
